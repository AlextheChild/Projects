{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import (\n",
    "    Flatten,\n",
    "    Dense,\n",
    ")\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU available. \n",
      "No GPU available. \n"
     ]
    }
   ],
   "source": [
    "# check cpu and gpu\n",
    "\n",
    "num_cpus = len(tf.config.list_physical_devices(\"CPU\"))\n",
    "num_gpus = len(tf.config.list_physical_devices(\"GPU\"))\n",
    "\n",
    "if num_cpus > 0:\n",
    "    print(\"CPU available. \")\n",
    "else:\n",
    "    print(\"No CPU available. \")\n",
    "\n",
    "if num_gpus > 0:\n",
    "    print(\"GPU available. \")\n",
    "else:\n",
    "    print(\"No GPU available. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create image and bounding box list dictionary\n",
    "\n",
    "rows = open(\"data/data.csv\").read().strip().split(\"\\n\")\n",
    "\n",
    "images_and_box_lists = {}\n",
    "\n",
    "current_image = \"images/0.png\"\n",
    "temp_box_list = []\n",
    "for row in rows:\n",
    "    row = row.split(\",\")\n",
    "\n",
    "    if row[0] != current_image:\n",
    "        images_and_box_lists.update({current_image: temp_box_list})\n",
    "        current_image = row[0]\n",
    "        temp_box_list = []\n",
    "    temp_box_list.append([int(row[1]), int(row[2]), int(row[3]), int(row[4])])\n",
    "images_and_box_lists.update({current_image: temp_box_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process images and make labels list (1:00)\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "ious = []\n",
    "\n",
    "w = 320\n",
    "h = 360\n",
    "\n",
    "\n",
    "def intersection_over_union(box1, box2):\n",
    "    box1_x1 = box1[0]\n",
    "    box1_y1 = box1[1]\n",
    "    box1_w = box1[2]\n",
    "    box1_h = box1[3]\n",
    "\n",
    "    box1_x2 = box1_x1 + box1_w\n",
    "    box1_y2 = box1_y1 + box1_h\n",
    "\n",
    "    box2_x1 = box2[0]\n",
    "    box2_y1 = box2[1]\n",
    "    box2_w = box2[2]\n",
    "    box2_h = box2[3]\n",
    "\n",
    "    box2_x2 = box2_x1 + box2_w\n",
    "    box2_y2 = box2_y1 + box2_h\n",
    "\n",
    "    intersection_x1 = max(box1_x1, box2_x1)\n",
    "    intersection_y1 = max(box1_y1, box2_y1)\n",
    "    intersection_x2 = min(box1_x2, box2_x2)\n",
    "    intersection_y2 = min(box1_y2, box2_y2)\n",
    "\n",
    "    intersection_area = max(0, intersection_x2 - intersection_x1) * max(\n",
    "        0, intersection_y2 - intersection_y1\n",
    "    )\n",
    "\n",
    "    box1_area = box1_w * box1_h\n",
    "    box2_area = box2_w * box2_h\n",
    "\n",
    "    return intersection_area / float(box1_area + box2_area - intersection_area)\n",
    "\n",
    "\n",
    "for key in images_and_box_lists:\n",
    "    full_image = Image.open(\"data/\" + key)\n",
    "\n",
    "    # loop through windows\n",
    "    for window_y1 in range(0, 1800, h):\n",
    "        for window_x1 in range(0, 2880, w):\n",
    "            window_x2 = window_x1 + w\n",
    "            window_y2 = window_y1 + h\n",
    "\n",
    "            # process image\n",
    "            processed_image = full_image.crop(\n",
    "                [window_x1, window_y1, window_x2, window_y2]\n",
    "            ).resize([32, 36])\n",
    "            processed_image = np.asarray(processed_image).astype(\"float32\") / 255\n",
    "            images.append(processed_image)\n",
    "\n",
    "            # get label\n",
    "            window_box = [window_x1, window_y1, w, h]\n",
    "            max_iou = 0\n",
    "            for balloon_box in images_and_box_lists.get(key):\n",
    "                iou = intersection_over_union(window_box, balloon_box)\n",
    "                if iou > max_iou:\n",
    "                    max_iou = iou\n",
    "            ious.append(max_iou)\n",
    "            if max_iou > 0.2:\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data (8:30)\n",
    "\n",
    "from PIL import ImageDraw, ImageFont\n",
    "\n",
    "font = ImageFont.load_default(100)\n",
    "\n",
    "index = 0\n",
    "for path in [\"images/\", \"flipped_images/\"]:\n",
    "    for file in range(0, 200):\n",
    "        image = Image.open(\"data/\" + path + str(file) + \".png\")\n",
    "\n",
    "        # draw box\n",
    "        draw = ImageDraw.Draw(image)\n",
    "\n",
    "        for y in range(0, 1800, h):\n",
    "            for x in range(0, 2880, w):\n",
    "                draw.rectangle(((x, y), (x + 320, y + 360)), outline=\"black\", width=3)\n",
    "\n",
    "                if labels[index] == 1:\n",
    "                    draw.rectangle(((x, y), (x + 320, y + 360)), outline=\"red\", width=6)\n",
    "                    draw.text((x+70, y+180), str(round(ious[index], 2)), font = font, fill=\"black\", stroke_fill=\"white\", stroke_width=3)\n",
    "\n",
    "                index += 1\n",
    "\n",
    "        image.save(\"check_data/\" + path + str(file) + \".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg16\n",
    "\n",
    "vgg = VGG16(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False,\n",
    "    input_tensor=keras.layers.Input(shape=(36, 32, 3)),\n",
    ")\n",
    "\n",
    "vgg.trainable = False\n",
    "\n",
    "window_x1 = vgg.output\n",
    "window_x1 = Flatten()(window_x1)\n",
    "window_x1 = Dense(128, activation=\"relu\")(window_x1)\n",
    "window_x1 = Dense(64, activation=\"relu\")(window_x1)\n",
    "window_x1 = Dense(32, activation=\"relu\")(window_x1)\n",
    "window_x1 = Dense(1, activation=\"sigmoid\")(window_x1)\n",
    "model = keras.Model(inputs=vgg.input, outputs=window_x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "\n",
    "train_images, test_images = np.split(images, [int(len(images) * 0.8)])\n",
    "train_boxes, test_boxes = np.split(labels, [int(len(labels) * 0.8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 231ms/step - loss: 0.1120 - val_loss: 0.1180\n",
      "Epoch 2/20\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 295ms/step - loss: 0.0952 - val_loss: 0.1058\n",
      "Epoch 3/20\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 294ms/step - loss: 0.0857 - val_loss: 0.0960\n",
      "Epoch 4/20\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 305ms/step - loss: 0.0831 - val_loss: 0.0957\n",
      "Epoch 5/20\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 295ms/step - loss: 0.0689 - val_loss: 0.0915\n",
      "Epoch 6/20\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 293ms/step - loss: 0.0708 - val_loss: 0.0852\n",
      "Epoch 7/20\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 299ms/step - loss: 0.0636 - val_loss: 0.0856\n",
      "Epoch 8/20\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 200ms/step - loss: 0.0602 - val_loss: 0.0800\n",
      "Epoch 9/20\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 199ms/step - loss: 0.0587 - val_loss: 0.0791\n",
      "Epoch 10/20\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 201ms/step - loss: 0.0527 - val_loss: 0.0855\n",
      "Epoch 11/20\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 203ms/step - loss: 0.0546 - val_loss: 0.0756\n",
      "Epoch 12/20\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 228ms/step - loss: 0.0494 - val_loss: 0.0751\n",
      "Epoch 13/20\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 323ms/step - loss: 0.0498 - val_loss: 0.0853\n",
      "Epoch 14/20\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 397ms/step - loss: 0.0487 - val_loss: 0.0718\n",
      "Epoch 15/20\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 408ms/step - loss: 0.0481 - val_loss: 0.0700\n",
      "Epoch 16/20\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 402ms/step - loss: 0.0440 - val_loss: 0.0701\n",
      "Epoch 17/20\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 383ms/step - loss: 0.0449 - val_loss: 0.0687\n",
      "Epoch 18/20\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 307ms/step - loss: 0.0405 - val_loss: 0.0670\n",
      "Epoch 19/20\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 378ms/step - loss: 0.0359 - val_loss: 0.0669\n",
      "Epoch 20/20\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 381ms/step - loss: 0.0360 - val_loss: 0.0675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x29c68406990>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train ()\n",
    "\n",
    "# ! this doesn't account for distortions in the fed-in images like will be seen once we take in ss inputs\n",
    "\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_images,\n",
    "    train_boxes,\n",
    "    epochs=20,\n",
    "    validation_data=(test_images, test_boxes),\n",
    "    verbose=1,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 303ms/step - loss: 0.0620\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06746142357587814"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "model.evaluate(test_images, test_boxes, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "\n",
    "model.save(\"saved_models/rcnn_vgg_bad_20.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
