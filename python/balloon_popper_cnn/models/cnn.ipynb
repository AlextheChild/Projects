{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# references\n",
    "\n",
    "# refrenced from https://towardsdatascience.com/implementing-a-fully-convolutional-network-fcn-in-tensorflow-2-3c46fb61de3b\n",
    "# also https://pyimagesearch.com/2020/10/05/object-detection-bounding-box-regression-with-keras-tensorflow-and-deep-learning/\n",
    "# used https://www.robots.ox.ac.uk/~vgg/software/via/via_demo.html to annotate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import (\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    "    Flatten,\n",
    "    Dense,\n",
    ")\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU available. \n",
      "No GPU available. \n"
     ]
    }
   ],
   "source": [
    "# check gpu\n",
    "\n",
    "num_cpus = len(tf.config.list_physical_devices('CPU'))\n",
    "num_gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "if num_cpus > 0:\n",
    "    print(\"CPU available. \")\n",
    "else:\n",
    "    print(\"No CPU available. \")\n",
    "\n",
    "if num_gpus > 0:\n",
    "    print(\"GPU available. \")\n",
    "else:\n",
    "    print(\"No GPU available. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# box preparation\n",
    "\n",
    "rows = open(\"data/data.csv\").read().strip().split(\"\\n\")\n",
    "images = []\n",
    "boxes = []\n",
    "image_names = []\n",
    "for row in rows:\n",
    "    row = row.split(\",\")\n",
    "    images.append(row[0])\n",
    "    image_names.append(row[0])\n",
    "    boxes.append(\n",
    "        [\n",
    "            int(row[1]) / 2880,\n",
    "            int(row[2]) / 1800,\n",
    "            int(row[3]) / 2880,\n",
    "            int(row[4]) / 1800,\n",
    "        ]\n",
    "    )\n",
    "boxes = np.array(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image preperation\n",
    "\n",
    "image_resolution = 20\n",
    "for i in range(len(images)):\n",
    "    image = Image.open(\"data/\" + images[i])\n",
    "    image = image.resize(\n",
    "        [image.width // image_resolution, image.height // image_resolution]\n",
    "    )\n",
    "    image = np.asarray(image)\n",
    "    image = image.astype(\"float32\") / 255\n",
    "    images[i] = image\n",
    "images = np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset image copy arrays\n",
    "\n",
    "images_copy = images.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw processed train images with boxes\n",
    "\n",
    "for i in range(len(images_copy)):\n",
    "    # convert to PIL\n",
    "    img = images_copy[i]\n",
    "    img *= 255\n",
    "    img = np.uint8(img)\n",
    "    img = Image.fromarray(img)\n",
    "\n",
    "    # draw box\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    p1 = (int(boxes[i][0] * 2880//20), int(boxes[i][1] * 1800//20))\n",
    "    p2 = (int(boxes[i][2] * 2880//20) + p1[0], int(boxes[i][3] * 1800//20) + p1[1])\n",
    "    draw.rectangle((p1, p2), outline=\"red\")\n",
    "\n",
    "    img.save(\"boxed_images/\" + image_names[i][:-4] + \"uwu\" + str(i) + \".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different network models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my network\n",
    "\n",
    "# input layer\n",
    "input = keras.layers.Input(shape=(1800 // image_resolution, 2880 // image_resolution, 3))\n",
    "\n",
    "# processing layers\n",
    "x = Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\")(input)\n",
    "x = Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)  # gets rid of unneeded detail in the image\n",
    "x = Dropout(rate=0.2)(x)  # prevents reliance on certain pixels\n",
    "\n",
    "x = BatchNormalization()(x)  # recentering and rescaling\n",
    "x = Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(rate=0.2)(x)\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "x = Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(rate=0.2)(x)\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "x = Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(rate=0.2)(x)\n",
    "\n",
    "# output layers\n",
    "x = BatchNormalization()(x)\n",
    "x = Flatten()(x)  # makes the array 1 dimensional\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "output = Dense(4, activation=\"sigmoid\")(x)\n",
    "\n",
    "# actually create the model\n",
    "model = keras.Model(inputs=input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer learning with vgg16\n",
    "\n",
    "vgg = VGG16(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False,\n",
    "    input_tensor=keras.layers.Input(shape=(1800 // image_resolution, 2880 // image_resolution, 3)),\n",
    ")\n",
    "\n",
    "vgg.trainable = False\n",
    "\n",
    "x = vgg.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "x = Dense(4, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs=vgg.input, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "\n",
    "train_images, test_images = np.split(images, [int(len(images) * 0.8)])\n",
    "train_boxes, test_boxes = np.split(boxes, [int(len(boxes) * 0.8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 3s/step - loss: 0.0713 - val_loss: 0.0385\n",
      "Epoch 2/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 3s/step - loss: 0.0316 - val_loss: 0.0338\n",
      "Epoch 3/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 3s/step - loss: 0.0292 - val_loss: 0.0332\n",
      "Epoch 4/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 3s/step - loss: 0.0281 - val_loss: 0.0333\n",
      "Epoch 5/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 3s/step - loss: 0.0296 - val_loss: 0.0332\n",
      "Epoch 6/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 3s/step - loss: 0.0276 - val_loss: 0.0332\n",
      "Epoch 7/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 2s/step - loss: 0.0266 - val_loss: 0.0329\n",
      "Epoch 8/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 2s/step - loss: 0.0271 - val_loss: 0.0328\n",
      "Epoch 9/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1402s\u001b[0m 29s/step - loss: 0.0273 - val_loss: 0.0327\n",
      "Epoch 10/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 3s/step - loss: 0.0262 - val_loss: 0.0335\n",
      "Epoch 11/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 4s/step - loss: 0.0275 - val_loss: 0.0331\n",
      "Epoch 12/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 3s/step - loss: 0.0263 - val_loss: 0.0326\n",
      "Epoch 13/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 3s/step - loss: 0.0260 - val_loss: 0.0326\n",
      "Epoch 14/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 3s/step - loss: 0.0261 - val_loss: 0.0329\n",
      "Epoch 15/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 3s/step - loss: 0.0262 - val_loss: 0.0330\n",
      "Epoch 16/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 3s/step - loss: 0.0265 - val_loss: 0.0327\n",
      "Epoch 17/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 3s/step - loss: 0.0266 - val_loss: 0.0333\n",
      "Epoch 18/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 3s/step - loss: 0.0255 - val_loss: 0.0330\n",
      "Epoch 19/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 3s/step - loss: 0.0263 - val_loss: 0.0324\n",
      "Epoch 20/20\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 3s/step - loss: 0.0267 - val_loss: 0.0326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x26b99753a90>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.MeanSquaredError(reduction=\"sum_over_batch_size\", name=\"mse\"),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_images,\n",
    "    train_boxes,\n",
    "    epochs=20,\n",
    "    validation_data=(test_images, test_boxes),\n",
    "    verbose=1,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - loss: 0.0323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.03264449164271355"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "model.evaluate(\n",
    "    test_images,\n",
    "    test_boxes,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "\n",
    "model.save(\"models/vgg_good_20_20.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
